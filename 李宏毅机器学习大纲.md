# 李宏毅机器学习

## 回归问题
### 基本概念
+ 过拟合和欠拟合
+ 偏置与方差
+ 正则化
### 梯度下降
+ 梯度下降算法
+ 自适应梯度下降
+ 随机梯度下降
+ 特征缩放（参数量级相差较大）
+ 理论基础
+ 限制

## 分类问题
### 生成模型
优点：
+ 需要的数据量较少（数据量较少时效果更好）
+ 鲁棒性更好
#### 概率生成模型
通过μ1, μ2, N1, N2, Σ, 得到w和b
**概率分布的选择**
+ 普通 -> 采用高斯分布
+ 只有两个特征 -> 采用伯努利分布
+ 各特征独立 -> 采用朴素贝叶斯分类器

**后验概率**
P(C1|x)通过约分和换元可化为激活函数sigmoid
当Σ公用时，有：P(C1|x) = σ(wx + b)
### 判别模型
直接找到w和b
判别模型的准确率通常更高
#### 逻辑回归
**二分类**：与线性回归只相差一个激活函数sigmoid
**多分类**：采用softmax(在二分类情况下就是sigmoid)
**限制**：如异或问题，难以找到一条线来划分不同类别 => 通过串联多个逻辑回归模型来解决=> 神经网络

## 深度学习
### 发展
感知机 -> 多层感知机 -> 反向传播 -> RBM受限玻尔兹曼机 -> GPU
### 步骤
1. 定义一系列函数（神经网络）
2. 函数的拟合优度
3. 选出最优模型

需要决定隐层的结构（多少层、每层几个神经元） => 试错 + 直觉
设计网络结构 => 卷积神经网络（CNN）

矩阵运算可以用GPU加速

### 反向传播
有效率地进行梯度下降

### 结果不好，如何优化
验证集结果不好：
+ 提前停下
  在验证集损失函数值最小时停下
+ 正则化
  泛化特征，让损失函数更平滑
+ Dropout
  训练集每次更新参数之前随机丢掉一些神经元（输入或隐层神经元） => 神经网络变得更加细长 => 训练集结果会变差，但是验证集结果会变好
  验证/测试集不做dropout，并且每个参数要乘(1-p)(p为丢弃率)
  dropout是一种ensemble（集成）
  
训练集结果不好：
+ 改变激活函数
    **梯度消失问题**
    通常神经网络所用的激活函数是sigmoid函数，这个函数有个特点，就是能将负无穷到正无穷的数映射到0和1之间，并且对这个函数求导的结果是f′(x)=f(x)(1−f(x))。因此两个0到1之间的数相乘，得到的结果就会变得很小了。神经网络的反向传播是逐层对函数偏导相乘，因此当神经网络层数非常深的时候，最后一层产生的偏差就因为乘了很多的小于1的数而越来越小，最终就会变为0，从而导致层数比较浅的权重没有更新，这就是梯度消失。
    解决：
    1、逐层训练
    2、改变激活函数，如换成ReLU（计算更快、结合生物特性、等价于无穷个sigmoid加上偏置叠加的结果，并且可以解决梯度消失问题）
    为什么ReLU可以解决梯度消失问题：
    经过ReLU后一些输出会为0，就可以从网络中拿掉，只留下线性部分，从而使网络变成一个更瘦的线性网络（对特定的点表现线性，整体是非线性的），输入端的梯度就不会变小了。  
    => Leaky ReLU
    **Maxout**
    可以学出任何分段线性凸函数激活函数（ReLU是Maxout的特例）
+ 自适应学习率
    **RMSProp**
    **难以找到合适参数的原因**
    + 平原（plateau）
    + 鞍点（saddle point）
    + 局部最小（local minima）
    不过，其实神经网络没有那么多局部最小，或者局部最小其实就很接近全局最小了

    解决：
    将真实世界的惯性加入到神经网络中，加一个动量（momentum）
    **Momentum**
    **Adam (RMSProp + Momentum)**

### 为什么deep而不是fat
模组化 -> 需要的数据较少

deep 相比 fat 更有效

语音 -> 高斯混合模型GMM

### 端到端学习

## 卷积神经网络(CNN)

滤波器

## 递归神经网络(RNN)

长短期记忆网络 Long Short-Term Memory(LSTM) -> 可以解决梯度消失问题

门控循环单元 Gated Recurrent Unit(GRU) 比LSTM简单

RNN, LSTM 和结构化网络（HMM, CRF, Structed Perception/SVM）的优劣： 

+ 前者可以进行深度学习
+ 后者可以考虑整个句子，并且损失函数是错误率的上界。

## Transformer

self-attention

## ELMO, BERT, GPT

### ELMO(Embeddings from Language Model)

### BERT(Bidirectional Encoder Representations from Transfromers)

BERT = Encoder of Transformer

### GPT(Generative Pre-Training)



## 半监督学习
+ self-training

## 无监督学习词向量（Word Embedding）
+ Count-based
+ Prediction-based
  + Continous bag of word(CBOW) model
  + Skip-gram













## Seq2Seq
+ RNN（可以学到之前的信息）
  图片由像素组成，每个词对应一个像素 
# 李宏毅机器学习

## 回归问题
### 基本概念
+ 过拟合和欠拟合
+ 偏置与方差
+ 正则化
### 梯度下降
+ 梯度下降算法
+ 自适应梯度下降
+ 随机梯度下降
+ 特征缩放（参数量级相差较大）
+ 理论基础
+ 限制
+ 
## 分类问题
### 生成模型
优点：
+ 需要的数据量较少（数据量较少时效果更好）
+ 鲁棒性更好
#### 概率生成模型
通过μ1, μ2, N1, N2, Σ, 得到w和b
**概率分布的选择**
+ 普通 -> 采用高斯分布
+ 只有两个特征 -> 采用伯努利分布
+ 各特征独立 -> 采用朴素贝叶斯分类器

**后验概率**
P(C1|x)通过约分和换元可化为激活函数sigmoid
当Σ公用时，有：P(C1|x) = σ(wx + b)
### 判别模型
直接找到w和b
判别模型的准确率通常更高
#### 逻辑回归
**二分类**
与线性回归只相差一个激活函数sigmoid
**多分类**
采用softmax(在二分类情况下就是sigmoid)
**限制**
如异或问题，难以找到一条线来划分不同类别
=> 通过串联多个逻辑回归模型来解决=> 神经网络

## 深度学习
### 发展
感知机 -> 多层感知机 -> 反向传播 -> RBM受限玻尔兹曼机 -> GPU
### 步骤
1. 定义一系列函数（神经网络）
2. 函数的拟合优度
3. 选出最优模型

需要决定隐层的结构（多少层、每层几个神经元） => 试错 + 直觉
设计网络结构 => 卷积神经网络（CNN）

矩阵运算可以用GPU加速

### 反向传播
有效率地进行梯度下降

### 为什么deep而不是fat
模组化 -> 需要的数据较少

deep 相比 fat 更有效

语音 -> 高斯混合模型GMM

### 端到端学习
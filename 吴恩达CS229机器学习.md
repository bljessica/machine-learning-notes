---
export_on_save:
 html: true
---

# 吴恩达CS229机器学习

## 定义

+   Arthur Samuel: 使计算机无需明确编程即可进行学习的研究领域。
+   Tom Mitchell: 一个适当的学习问题定义如下：计算机程序从经验E中学习，解决某一任务 T ，进行某一性能度量 P ，通过 P 测定在 T 上的表现因经验 E 而提高。

## 术语

+   回归：设法预测连续值输出
+   分类：设法预测离散值输出
+   数据集：一组记录的集合
+   正类：y 有两个取值的情况下用 1 来表示的这一类（表示具有我们要找的东西）
+   负类：y 有两个取值的情况下用 0 来表示的这一类（表示没有我们要找的东西）
+   聚类算法：把数据分成不同的簇
+   鸡尾酒会算法：多个人对着多个麦克风说话，分离人声和音乐(解决方法为 ICA ，独立组件分析)
+   参数学习算法：有一组固定的参数（如$\theta$~i~）
+   非参数学习算法：数据或参数的数量持续增长 

## 符号

-   m: 训练样本的数量
-   n：特征的数量
-   x: 输入变量/特征
-   y: 输出/目标变量
-   $(x, y)$: 一个训练样本
-   $h_\theta(x)$(或$h(x)$): 假设函数
-   $x^{(i)}$表示第 i 个训练样本（特征向量）
-   $x^{(i)}_j$表示第 i 个训练样本（特征向量）中第 j 个特征向量的值

## 学习算法分类

### 监督学习（supervised learning）

对于监督学习的每一个样本，我们都得到了输入 x 和输出 y ，希望找到一个 x 到 y 的映射，以便当给予新的 x 时可以得到 y 。

#### 1.线性回归（linear regression, 回归算法）

（有时通过简单变形如代换平方根，平方即可转换为线性回归）

线性回归的代价函数总是弓状函数（凸函数），没有局部最优解，只有一个全局最优。

假设函数：$h_\theta(x) = X\theta$

代价函数：$$J(\theta) = \frac{1}{2} \sum_{i=1}^m (h_\theta x^{(i)} - y^{(i)})^2$$

目标：找寻使得代价函数最小的$\theta$。

##### 1.梯度下降算法

+   批量梯度下降算法(Batch gradient descent)
	$$\theta_j := \theta_j - \alpha \sum_{i=1}^m(h_\theta x^{(i)} - y^{(i)})x^{(i)}_j  (j = 0, 1, ..., n)$$

​	每次迭代都要遍历整个数据集，因此不适用于数据集很大的情况。

+   随机梯度下降算法(Stochastic gradient descent)
	$$\theta_j := \theta_j - \alpha (h_\theta x^{(i)} - y^{(i)})x^{(i)}_j  (j = 0, 1, ..., n)$$

    永远不会收敛，但会接近全局最优，适用于在数据集很大时使算法快速取得进展。

##### 2.正规方程法(normal equation)

提供一种求$\theta$的解析解法，无需迭代，可以一次性求解$\theta$的最优值。

推导：

$$
\begin{aligned}
J(\theta) &= 
\frac{1}{2} \sum_{i=1}^m (h_\theta x^{(i)} - y^{(i)})^2	\\
	&= \frac{1}{2}(X \theta - y)^T(X\theta - y)\\
	&= \frac{1}{2}(\theta^TX^T - y^T)(X\theta - y)\\
	&= \frac{1}{2}(\theta^TX^TX\theta - \theta^T X^Ty - y^TX\theta + y^Ty)
\end{aligned}
$$
要使$J(\theta)$对$\theta$的导数为0（矩阵求导），则：
	$$ \frac{1}{2}(X^TX\theta + X^TX\theta - X^Ty - X^Ty) = 0$$
	求解得到：
	$$\theta = (X^TX)^{-1}X^Ty$$

矩阵不可逆的情况：
-   有多余的变量（线性相关）
-   即使矩阵不可逆，使用伪逆函数也能得到正确的结果

##### 比较梯度下降法和正规方程法

梯度下降法

-   需要选择α
-   需要多次迭代
-   在特征变量很多的情况下也能运行得很好

正规方程法

-   不需要选择α
-   不需要多次迭代
-   在特征变量很多(>10000)的情况下会很慢（O(n^3^)）

##### 局部加权回归(locally weighted regression)

一种修改线性回归使得可以适用于非线性回归的方法。

##### 线性回归的概率

#### 2.Logistic回归（分类算法）

**sigmoid函数/logistic函数**：1/(1 + e^-z^)（值介于(0, 1)之间）

假设函数h~θ~(x) = g($\theta$^T^x) = 1/(1 + e^-θTx^)，介于[0, 1]之间

h~θ~x = P(y = 1|x;$\theta$)的意思是参数为$\theta$，对具有特征x的样本，y = 1的概率

根据sigmoid函数的图像可得，只要$\theta$^T^x >= 0,则预测y = 1；只要$\theta$^T^x < 0,则预测y = 0

**决策边界（是假设函数和参数$\theta$的属性）：**将整个平面分为两部分的线

如何拟合$\theta$：

**决策界限**

##### 二分类问题

##### 多分类问题

### 无监督学习（让计算机自己学习）

我们得到的是无标签的数据，即只得到输入x, 要求将数据分类。

## Octave编程语言

### 1.基本操作

%：注释

~=：不等于

PS1('>>')：更改提示符样式为>>

;：阻止命令行输出，矩阵换行

disp()：控制输出格式

format long/short：改变显示默认类型

v = [1 2; 3 4] // 2*2

v = 1:0.1:2 //[1 1.1 1.2 ... 2]

v = 1:6 // [1 2 3 4 5 6]

n * ones(2,3)：生成一个2*3且值全为n的矩阵

zeros(2,3)：生成一个2*3且值全为0的矩阵

rand(2,3)：生成一个2*3且值全为0~1之间的随机数的矩阵

randn(2,3)：生成一个2*3且值遵从高斯分布（均值为0，标准差为1）的矩阵

hist(v)：将v绘制成直方图

eye(n)：生成一个n*n的单位矩阵

help 函数名：函数有关的帮助文档

size(A)：返回矩阵A的维度// 如2 3

length(A)：返回矩阵A的最大维度的大小

pwd：显示当前路径

who：显示所有变量

whos：显示所有变量及详细信息

clear(A)或clear A：删除变量A

clear：删除所有变量

v = A(1:10)：将A的前十个元素赋给v

save hello.mat v：将v保存为名为hello.mat的文件

save hello.txt v -ascii：将v保存为文本/ascii编码的文档

### 2.移动数据

load features.dat或load('features.dat')：读入文件（变量名为features）

A(3,2)：A矩阵第三行第二列的元素

A(2,;)：A矩阵第二行的所有元素

A([1 3],;)：A矩阵一、三行的所有元素

A = [A, [1; 2; 3]]：在A的最右端加一列

A(:)：将A的所有元素放入一个列向量（一列一列放）

### 3.计算数据

A * C：矩阵相乘

A .* C：每个元素对应相乘（.一般表示元素的运算）

A .^ 2：A的每个元素进行乘方

1 ./ A：A的每个元素取倒数

log(A)：对A中所有元素求对数

exp(A)：对A中所有元素求指数

abs(A)：对A中所有元素求绝对值

A + 1：A的每个元素都加一

A'：A的转置

max(A)：A中每一列最大的元素

max(max(A))：矩阵A的最大值

[val, index] = max(a)：val为值，index为索引，a为向量

A < 3：元素比较，结果为矩阵

find(A < 3)：返回所有小于3的元素

magic(3)：生成一个3*3的幻方矩阵（每行、每列、对角线的和都一样）

sum(A)：A中所有元素的和

sum(A, 1)：A中每一列的和

prod(A)：A中所有元素的乘积

floor(A)：对A中所有元素向下取整

ceil(A)：对A中所有元素向上取整

flipud(A)：使矩阵垂直翻转

### 4.数据绘制

plot(t, y1)：绘制三角函数//t为行向量,y1为函数，可以有第三个参数颜色，如'r'

hold on：在旧图像上继续绘制新图像

xlabel('time')：设置横轴标签

ylabel('value')：设置纵轴标签

legend('sin', 'cos')：标记曲线

title('my plot')：设置标题

print -dpng 'myPlot.png'：保存图像

close：关闭图像

figure(1)；plot(t, y1)：绘制第一个图

subplot(1,5,2)：将图像分为1*5的格子，使用第2个格子（就可以将多幅图画在一个窗口上）

axis([0.5 1 -1 1])：设置显示范围（x轴，y轴）

clf：清除一幅图像

imagesc(A)：将图像分为m*n格，对应矩阵的每一个值

### 5.控制语句

-   for

```
for i=1:10,
	v(i) = 2 ^ i;
end;
```

-   while、break

```
while i <= 5,
	v(i) = 100;
	i = i + 1;
	if i == 2,
		break;
	end;
end;
```

-   if、elseif、else

```
if v(i) == 1,
	disp('one');
elseif v(i) == 2,
	disp('two');
else
	disp('neither');
end;
```

-   函数(可有多个返回值)

```
function [y1, y2] = square (x) 
y1 = x^2;
y2 = x^3;
```

`addPath('')`：添加搜索路径
# 吴恩达CS229机器学习

## 定义

+   Arthur Samuel: 使计算机无需明确编程即可进行学习的研究领域。
+   Tom Mitchell: 一个适当的学习问题定义如下：计算机程序从经验E中学习，解决某一任务 T ，进行某一性能度量 P ，通过 P 测定在 T 上的表现因经验 E 而提高。

## 术语

+   回归：设法预测连续值输出
+   分类：设法预测离散值输出
+   数据集：一组记录的集合
+   正类：y 有两个取值的情况下用 1 来表示的这一类（表示具有我们要找的东西）
+   负类：y 有两个取值的情况下用 0 来表示的这一类（表示没有我们要找的东西）
+   聚类算法：把数据分成不同的簇
+   鸡尾酒会算法：多个人对着多个麦克风说话，分离人声和音乐(解决方法为 ICA ，独立组件分析)
+   参数学习算法：有一组固定的参数（如$\theta_i$）
+   非参数学习算法：数据或参数的数量持续增长 
+   LMS：最小二乘法
+   MLE：极大似然估计

## 符号

-   m: 训练样本的数量
-   n：特征的数量
-   x: 输入变量/特征
-   y: 输出/目标变量
-   ![](http://latex.codecogs.com/svg.latex?(x,y)) : 一个训练样本
-   ![](http://latex.codecogs.com/svg.latex?h_\theta(x)) 或![](http://latex.codecogs.com/svg.latex?h(x))) : 假设函数
-   ![](http://latex.codecogs.com/svg.latex?x^{(i)})表示第 i 个训练样本（特征向量）
-   ![](http://latex.codecogs.com/svg.latex?x^{(i)}_j)表示第 i 个训练样本（特征向量）中第 j 个特征向量的值

## 学习算法分类

### 监督学习（supervised learning）

对于监督学习的每一个样本，我们都得到了输入 x 和输出 y ，希望找到一个 x 到 y 的映射，以便当给予新的 x 时可以得到 y 。

#### 1.线性回归（Linear regression, 回归算法）

（有时通过简单变形如代换平方根，平方即可转换为线性回归）

线性回归的代价函数总是弓状函数（凸函数），没有局部最优解，只有一个全局最优。

假设函数：![](http://latex.codecogs.com/svg.latex?h_\theta(x)=X\theta)

代价函数：

![](http://latex.codecogs.com/svg.latex?J(\theta)=\frac{1}{2}\sum_{i=1}^m(h_\theta\{x}^{(i)}-y^{(i)})^2)

目标：找寻使得代价函数最小的$\theta$。

##### 1.梯度下降算法

+   批量梯度下降算法(Batch gradient descent, BGD)  
  
   ![](http://latex.codecogs.com/svg.latex?\theta_j:=\theta_j-\\alpha\sum_{i=1}^m(h_\theta\{x}^{(i)}-y^{(i)})x^{(i)}_j(j=0,1,...,n))

​	每次迭代都要遍历整个数据集，因此不适用于数据集很大的情况。

+   随机梯度下降算法(Stochastic gradient descent, SGD)
   
	![](http://latex.codecogs.com/svg.latex?\theta_j:=\theta_j-\alpha(h_\theta\{x}^{(i)}-y^{(i)})x^{(i)}_j(j=0,1,...,n))

    永远不会收敛，但会接近全局最优，适用于在数据集很大时使算法快速取得进展。

##### 2.正规方程法(normal equation)

提供一种求![](http://latex.codecogs.com/svg.latex?\theta)的解析解法，无需迭代，可以一次性求解![](http://latex.codecogs.com/svg.latex?\theta)的最优值。

推导：

![](http://latex.codecogs.com/svg.latex?\begin{aligned}J(\theta)&=\frac{1}{2}\sum_{i=1}^m(h_\theta\{x}^{(i)}-y^{(i)})^2\\\\&=\frac{1}{2}(X\theta-y)^T(X\theta-y)\\\\&=\frac{1}{2}(\theta^TX^T-y^T)(X\theta-y)\\\\&=\frac{1}{2}(\theta^TX^TX\theta-\theta^T\{X}^T\{y}-y^TX\theta+y^Ty)\end{aligned})

要使![](http://latex.codecogs.com/svg.latex?J(\theta))对![](http://latex.codecogs.com/svg.latex?\theta)的导数为0（矩阵求导），则：  

![](http://latex.codecogs.com/svg.latex?\frac{1}{2}(X^TX\theta+X^TX\theta-X^Ty-X^Ty)=0)

求解得到：  

![](http://latex.codecogs.com/svg.latex?\theta=(X^TX)^{-1}X^Ty)

矩阵不可逆的情况：
-   有多余的变量（线性相关）
-   即使矩阵不可逆，使用伪逆函数也能得到正确的结果

##### 比较梯度下降法和正规方程法

梯度下降法

-   需要选择α
-   需要多次迭代
-   在特征变量很多的情况下也能运行得很好

正规方程法

-   不需要选择α
-   不需要多次迭代
-   在特征变量很多(>10000)的情况下会很慢（O(n^3^)）

##### 局部加权线性回归(locally weighted linear regression)

一种修改线性回归使得可以适用于非线性回归的方法，适用于数据量较小的情况（如几千）。

思想：选择要预测的 x 附近（在 x 轴上的值接近）的训练示例进行拟合。

代价函数：

​	![](http://latex.codecogs.com/svg.latex?J(\theta)=\sum^m_{i=1}\omega^{(i)}(y^{(i)}-\theta^Tx^{(i)})^2)

其中，加权函数：![](http://latex.codecogs.com/svg.latex?\omega^{(i)}=e^{-\frac{(x^{(i)}-x)^2}{2\tau^2}})

如果![](http://latex.codecogs.com/svg.latex?|x^{(i)}-x|)小，则![](http://latex.codecogs.com/svg.latex?\omega^{(i)}\approx1)，如果![](http://latex.codecogs.com/svg.latex?|x^{(i)}-x|)大，则![](http://latex.codecogs.com/svg.latex?\omega^{(i)}\approx0)。参数![](http://latex.codecogs.com/svg.latex?\tau)控制了权值变化的速率。

##### 线性回归的概率解释（为什么使用平方误差？）

[线性回归概率解释](https://blog.csdn.net/sz464759898/article/details/43957433)

#### 2.逻辑回归（Logistic regression, 分类算法）

假设函数![](http://latex.codecogs.com/svg.latex?h_{\theta}(x)=g(\theta^T\{x})=\frac{1}{1+e^{-\theta^T\{x}}})，介于[0, 1]之间

**sigmoid/logistic函数**：![](http://latex.codecogs.com/svg.latex?g(x)=\frac{1}{1+e^{-x}})（值介于(0, 1)之间）

（![](http://latex.codecogs.com/svg.latex?h_{\theta}(x)=P(y=1|x;{\theta}))表示参数为θ，对具有特征x的样本，y = 1的概率）

![](http://latex.codecogs.com/svg.latex?L(\theta)=\sum_{i=1}^m\{y}^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)})))

##### 1.批梯度上升法：(Batch gradient ascent)

![](http://latex.codecogs.com/svg.latex?\theta_j=\theta_j+\alpha\sum_{i=1}^m(y^{(i)}-h_\theta\{x}^{(i)})x_j^{(i)})

##### 2.牛顿法(Newton's method)

迭代步伐更大（具有二阶收敛的特性），但每次迭代更昂贵，适用于参数数量较少的情况。

每次迭代后 θ 更新为 x = θ 与函数交点处的切线与 x 轴的交点。

![](http://latex.codecogs.com/svg.latex?\theta^{(t+1)}=\theta^{(t)}-\frac{L'(\theta^{(t)})}{L''(\theta^{(t)})})（θ为实数）

![](http://latex.codecogs.com/svg.latex?\theta:=\theta-H^{-1}\nabla_{\theta}\ell(\theta))（θ为矢量），其中▽θ表示对矩阵每个元素求偏导数，H为Hessian矩阵，其矩阵元素为![](http://latex.codecogs.com/svg.latex?H_{ij}=\frac{\partial^{2}\ell(\theta)}{\partial\theta_{i}\partial\theta_{j}})

##### 感知机（Perceptron）

找寻一个平面来分离 y = 0 和 y = 1 的样本

![](http://latex.codecogs.com/svg.latex?g(z)=\begin{cases}1,z\geq0\\\\0,z<0\end{cases})

![](http://latex.codecogs.com/svg.latex?h_\theta(x)=g(\theta^Tx))

![](http://latex.codecogs.com/svg.latex?\theta_j=\theta_j+\alpha(y^{(i)}-h_\theta\{x}^{(i)})x_j^{(i)})，更新法向量，从而更新平面

##### 指数族分布(Exponential familiy)

概率密度函数（PDF）：![](http://latex.codecogs.com/svg.latex?P(y;\eta)=b(y)\exp[\eta^TT(y)-\alpha(\eta)]),η为自然参数，T(y)为充分统计量，b(y)为基本措施，α(η)为log配分函数

可以得到，伯努利分布、高斯分布服从指数族分布。

根据需要的 y 的类型选择：
实数-选择高斯分布
二进制(如0, 1)-选择伯努利分布
计数-泊松分布
正实数-伽马/指数分布
概率分布-beta/狄利克雷（主要出现在贝叶斯统计中）

特性：
+ 关于η的MLE（极大似然估计）是凹的，NLL(negative log-likelihood)是凸的
+ ![](http://latex.codecogs.com/svg.latex?E(y;\eta)=\frac{\partial}{\partial\eta}\alpha(\eta))
+ ![](http://latex.codecogs.com/svg.latex?D(y;\eta)=\frac{\partial^2}{\partial^2\eta}\alpha(\eta))（方差）

##### 广义线性模型(Generalized linear models, GLM)

迭代规则：![](http://latex.codecogs.com/svg.latex?\theta_j=\theta_j+\alpha(y^{(i)}-h_\theta\{x}^{(i)})x_j^{(i)})

![](http://latex.codecogs.com/svg.latex?\mu=E[y;\eta]=g(\eta)(g(\eta)=\frac{\partial}{\partial\eta}\alpha(\eta)))为规范响应函数

![](http://latex.codecogs.com/svg.latex?\eta=g^{-1}(\mu))为规范链接函数


三种参数化：
+ 模型参数θ（--θ^Tx-->η）
+ 自然参数η（--g-->标准参数）
+ 标准参数：（--g^-1-->η）
  + φ-伯努利
  + μσ^2-高斯
  + λ-泊松

##### softmax回归（属于GLM）

一种非GLM的方法——交叉熵最小化

每一类对应一个θ

##### 之前的全为判别方法，以下为生成方法

##### 高斯判别分析（Gaussian Discriminant analysis, GDA）

##### 生成方法和判别方法的比较

生成方法：分别为每个类建立模型（几乎是独立的）（最大化条件似然）

判别方法：同时查看每个类并寻找一种将他们分开的方法（最大化条件可能性）

##### 朴素贝叶斯

### 无监督学习（让计算机自己学习）

我们得到的是无标签的数据，即只得到输入x, 要求将数据分类。

## Octave编程语言

### 1.基本操作

%：注释

~=：不等于

PS1('>>')：更改提示符样式为>>

;：阻止命令行输出，矩阵换行

disp()：控制输出格式

format long/short：改变显示默认类型

v = [1 2; 3 4] // 2*2

v = 1:0.1:2 //[1 1.1 1.2 ... 2]

v = 1:6 // [1 2 3 4 5 6]

n * ones(2,3)：生成一个2*3且值全为n的矩阵

zeros(2,3)：生成一个2*3且值全为0的矩阵

rand(2,3)：生成一个2*3且值全为0~1之间的随机数的矩阵

randn(2,3)：生成一个2*3且值遵从高斯分布（均值为0，标准差为1）的矩阵

hist(v)：将v绘制成直方图

eye(n)：生成一个n*n的单位矩阵

help 函数名：函数有关的帮助文档

size(A)：返回矩阵A的维度// 如2 3

length(A)：返回矩阵A的最大维度的大小

pwd：显示当前路径

who：显示所有变量

whos：显示所有变量及详细信息

clear(A)或clear A：删除变量A

clear：删除所有变量

v = A(1:10)：将A的前十个元素赋给v

save hello.mat v：将v保存为名为hello.mat的文件

save hello.txt v -ascii：将v保存为文本/ascii编码的文档

### 2.移动数据

load features.dat或load('features.dat')：读入文件（变量名为features）

A(3,2)：A矩阵第三行第二列的元素

A(2,;)：A矩阵第二行的所有元素

A([1 3],;)：A矩阵一、三行的所有元素

A = [A, [1; 2; 3]]：在A的最右端加一列

A(:)：将A的所有元素放入一个列向量（一列一列放）

### 3.计算数据

A * C：矩阵相乘

A .* C：每个元素对应相乘（.一般表示元素的运算）

A .^ 2：A的每个元素进行乘方

1 ./ A：A的每个元素取倒数

log(A)：对A中所有元素求对数

exp(A)：对A中所有元素求指数

abs(A)：对A中所有元素求绝对值

A + 1：A的每个元素都加一

A'：A的转置

max(A)：A中每一列最大的元素

max(max(A))：矩阵A的最大值

[val, index] = max(a)：val为值，index为索引，a为向量

A < 3：元素比较，结果为矩阵

find(A < 3)：返回所有小于3的元素

magic(3)：生成一个3*3的幻方矩阵（每行、每列、对角线的和都一样）

sum(A)：A中所有元素的和

sum(A, 1)：A中每一列的和

prod(A)：A中所有元素的乘积

floor(A)：对A中所有元素向下取整

ceil(A)：对A中所有元素向上取整

flipud(A)：使矩阵垂直翻转

### 4.数据绘制

plot(t, y1)：绘制三角函数//t为行向量,y1为函数，可以有第三个参数颜色，如'r'

hold on：在旧图像上继续绘制新图像

xlabel('time')：设置横轴标签

ylabel('value')：设置纵轴标签

legend('sin', 'cos')：标记曲线

title('my plot')：设置标题

print -dpng 'myPlot.png'：保存图像

close：关闭图像

figure(1)；plot(t, y1)：绘制第一个图

subplot(1,5,2)：将图像分为1*5的格子，使用第2个格子（就可以将多幅图画在一个窗口上）

axis([0.5 1 -1 1])：设置显示范围（x轴，y轴）

clf：清除一幅图像

imagesc(A)：将图像分为m*n格，对应矩阵的每一个值

### 5.控制语句

-   for

```
for i=1:10,
	v(i) = 2 ^ i;
end;
```

-   while、break

```
while i <= 5,
	v(i) = 100;
	i = i + 1;
	if i == 2,
		break;
	end;
end;
```

-   if、elseif、else

```
if v(i) == 1,
	disp('one');
elseif v(i) == 2,
	disp('two');
else
	disp('neither');
end;
```

-   函数(可有多个返回值)

```
function [y1, y2] = square (x) 
y1 = x^2;
y2 = x^3;
```

`addPath('')`：添加搜索路径